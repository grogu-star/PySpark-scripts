{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[2]\") \\\n",
    "                    .appName(\"spark-course\") \\\n",
    "                    .config(\"spark.driver.memory\", \"1024m\") \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.ml.param.shared import HasOutputCol, HasInputCol\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, StringType, ArrayType, MapType, IntegerType\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -h /labs/slaba04/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = spark.read.csv('/labs/slaba04/gender_age_dataset.txt', sep = '\\t', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema_of_json('{\"visits\": [{\"url\": \"http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\", \"timestamp\": 1419688144068}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426666298001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426666298000}, {\"url\": \"http://news.yandex.ru/yandsearch?cl4url=chezasite.com/htc/htc-one-m9-delay-86327.html&lr=213&rpt=story\", \"timestamp\": 1426661722001}, {\"url\": \"http://www.sotovik.ru/news/240283-htc-one-m9-zaderzhivaetsja.html\", \"timestamp\": 1426661722000}]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dt.select(from_json(col(\"user_json\").cast(\"string\"), schema = schema).alias(\"s\"), 'gender', 'age', 'uid') \\\n",
    "    .select(col(\"s.*\"), 'gender', 'age', 'uid')\n",
    "df.show(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(ArrayType(StringType()))\n",
    "def get_url_array(x):\n",
    "    y = []\n",
    "    for i in x:\n",
    "        g = re.search(r'(?<=http://)[\\w\\.-]+|(?<=https://)[\\w\\.-]+', i[1])\n",
    "        if g:\n",
    "            url = g.group(0)\n",
    "            y.append(url)\n",
    "    return y\n",
    "\n",
    "df2 = df.withColumn('url_array', get_url_array(df.visits))\n",
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#топ сайтов\n",
    "gdf_ = df2.select('uid', explode(df2.url_array).alias('url')).distinct().groupBy('url').agg(count('url').alias('cnt'))\n",
    "#gdf_.show(3)\n",
    "gdf2_ = gdf_.repartition(1)\n",
    "gdf3_ = gdf2_.orderBy('cnt', ascending=False).limit(500)\n",
    "gdf3_cache = gdf3_.cache()\n",
    "gdf3_cache.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.select('uid', 'age', 'gender', explode(df2.url_array).alias('url')) \\\n",
    "      .join(gdf3_cache, on='url', how='inner') \\\n",
    "      .groupBy('uid', 'age', 'gender') \\\n",
    "      .agg(collect_list('url').alias('url'))\n",
    "\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(inputCol='url', outputCol='url_vector')\n",
    "count_vectorizer_model = count_vectorizer.fit(df3)\n",
    "df4 = count_vectorizer_model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4[(df4.age.isin('>=55', '45-54', '35-44', '25-34', '18-24')) & (df4.gender.isin('F', 'M'))] \\\n",
    "    ['gender', 'age', 'url_vector']\n",
    "\n",
    "df6 = \\\n",
    "(\n",
    "df5\n",
    "    .withColumn('age_', when(col('age') == '18-24', 1)\n",
    "                      .when(col('age') == '25-34', 2)\n",
    "                      .when(col('age') == '35-44', 3)\n",
    "                      .when(col('age') == '45-54', 4)\n",
    "                      .when(col('age') == '>=55', 5)\n",
    "                      )\n",
    "    .withColumn('gender_', when(col('gender') == 'F', 1)\n",
    "                          .when(col('gender') == 'M', 0)\n",
    "                          )\n",
    ")\n",
    "df6_cache = df6.cache()\n",
    "df6_cache.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(IntgerType)\n",
    "def url_num(x):\n",
    "    return len(x)\n",
    "\n",
    "df2_1 = df2.withColumn('url_num', url_num(df2.url_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "model_age = RandomForestClassifier(featuresCol='url_vector', labelCol='age_').fit(df6_cache)\n",
    "model_gender = GBTClassifier(featuresCol='url_vector', labelCol='gender_').fit(df6_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Из кафки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline\n",
    "def transform_batch(kafka_sdf):\n",
    "    \n",
    "    data0 = kafka_sdf.select(\n",
    "    json_tuple(col(\"value\").cast(\"string\"), \"uid\", \"visits\").alias(\"uid\", \"user_js\"))\n",
    "\n",
    "    data = data0.select('uid', 'user_js', concat(lit('{\"visits\": '), col('user_js'), lit('}')) \\\n",
    "            .alias('user_json'))\n",
    "\n",
    "    data1 = data.select(\n",
    "        from_json(col(\"user_json\").cast(\"string\"), schema = schema).alias(\"s\"),\n",
    "        'uid'\n",
    "    ).select(col(\"s.*\"),'uid')\n",
    "    \n",
    "    data2 = data1.withColumn('url', get_url_array(data1.visits))\n",
    "    \n",
    "    data5 = count_vectorizer_model.transform(data2)\n",
    "    \n",
    "    data6 = model_age.transform(data5).select('uid', 'url_vector', col('prediction').alias('age_'))\n",
    "    data7 = model_gender.transform(data6).select('uid', 'age_', col('prediction').alias('gender_'))\n",
    "    data8 = data7\\\n",
    "            .withColumn('age', when(col('age_') == 1, '18-24')\n",
    "                              .when(col('age_') == 2, '25-34')\n",
    "                              .when(col('age_') == 3, '35-44')\n",
    "                              .when(col('age_') == 4, '45-54')\n",
    "                              .when(col('age_') == 5, '>=55')\n",
    "                      )\\\n",
    "           .withColumn('gender', when(col('gender_') == 1, 'F')\n",
    "                                .when(col('gender_') == 0, 'M')\n",
    "                          ) \\\n",
    "           .select('uid', 'age', 'gender')\n",
    "    return data8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#чтение из кафка\n",
    "read_kafka_params = {\n",
    "    \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "    \"subscribe\": \"input_margarita.cherentsova\",\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "kafka_sdf = spark.readStream.format(\"kafka\").options(**read_kafka_params).load()\n",
    "\n",
    "#модель\n",
    "transfoem_kafka = transform_batch(kafka_sdf)\n",
    "\n",
    "#запись\n",
    "\n",
    "kafka_doc = to_json(struct(col(\"*\")))\n",
    "raw = transfoem_kafka.select(kafka_doc.alias(\"value\")) \\\n",
    "\n",
    "#запись в кафка\n",
    "write_kafka_params = {\n",
    "   \"kafka.bootstrap.servers\": 'spark-master-1.newprolab.com:6667',\n",
    "   \"topic\": \"margarita.cherentsova\"\n",
    "}\n",
    "raw.writeStream.format(\"kafka\").options(**write_kafka_params)\\\n",
    "    .option(\"checkpointLocation\", \"streaming/chk/chk_kafka\")\\\n",
    "    .outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kill_all():\n",
    "    streams = SparkSession.builder.getOrCreate().streams.active\n",
    "    if streams:\n",
    "        for s in streams:\n",
    "            desc = s.lastProgress[\"sources\"][0][\"description\"]\n",
    "            s.stop()\n",
    "            print(\"Running stream {s}\".format(s=desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kill_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
